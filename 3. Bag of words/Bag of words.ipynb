{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c503f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e74ab",
   "metadata": {},
   "source": [
    "## 1. 데이터 읽기, 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25ec65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.<br/>과 같은 html 태그 삭제\n",
    "# 2. 단어만을 남기고 문장부호나 숫자들을 삭제(regular expression)\n",
    "# 3. nltk 패키지로 stopword 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dcff5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "#!pip install nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cfe7c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\taeeu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7be1058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 생성\n",
    "def review_to_words(raw_review):\n",
    "  review_text=BeautifulSoup(raw_review, features=\"html.parser\").get_text()\n",
    "  letters_only=re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "  words=letters_only.lower().split()\n",
    "  stops=set(stopwords.words(\"english\"))\n",
    "  meaningful_words=[w for w in words if not w in stops]\n",
    "\n",
    "  return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de9820f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실제로 train 데이터를 하나씩 review_to_words 함수를 통해서 clean_train_reviews\n",
    "num_reviews=train[\"review\"].size\n",
    "clean_train_reviews=[]\n",
    "\n",
    "for i in range(0, num_reviews):\n",
    "  if ((i+1) % 1000==0):\n",
    "    print(\"Review %d of %d\\n\" % (i + 1, num_reviews))\n",
    "  clean_train_reviews.append(review_to_words(train[\"review\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0fc31d",
   "metadata": {},
   "source": [
    "## 2. Bag of Words\n",
    "- Frequency based Embedding(단어로 이루어진 데이터를 수치화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf51b964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\taeeu\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\taeeu\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\taeeu\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\taeeu\\anaconda3\\lib\\site-packages (from scikit-learn) (1.22.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\taeeu\\anaconda3\\lib\\site-packages (from scikit-learn) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c36c4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "761563ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어중 빈도가 높은 5000개만 사용\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer=None, preprocessor = None, stop_words=None, max_features=5000)\n",
    "\n",
    "# fit_transform=vectorizer가 우리의 vocabulary를 익힘\n",
    "train_data_features=vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# 다루기 쉽게 array로 변환\n",
    "train_data_featrues=train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fda472",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67ffe9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3255dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest=RandomForestClassifier(n_estimators=100)\n",
    "forest=forest.fit(train_data_features, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d775a4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "test=pd.read_csv(\"testData.tsv\", header=0, delimiter='\\t', quoting=3)\n",
    "num_reviews=len(test[\"review\"])\n",
    "print(num_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6547b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_reviews=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9b8ba86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_reviews):\n",
    "    if ((i+1) % 1000 == 0):\n",
    "        print(\"Review %d of %d\\n\" % (i + 1, num_reviews))\n",
    "    clean_review=review_to_words(test[\"review\"][i])\n",
    "    clean_test_reviews.append(clean_review)\n",
    "test_data_features=vectorizer.transform(clean_test_reviews)\n",
    "test_data_features=test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39ba95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=forest.predict(test_data_features)\n",
    "output=pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": result})\n",
    "\n",
    "output.to_csv(\"Bag_of_Words_model.csv\", index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
